{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb4fb9e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This sample application demonstrates how to download models from Open Model Zoo which are validated with Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bc726",
   "metadata": {},
   "source": [
    "### Print a list of all available models on Open Model Zoo\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the Model Downloader utility to download some common inference models from the Open Model Zoo.\n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the --print_all argument to see all the available inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c97d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action-recognition-0001-decoder\n",
      "action-recognition-0001-encoder\n",
      "age-gender-recognition-retail-0013\n",
      "asl-recognition-0004\n",
      "bert-large-uncased-whole-word-masking-squad-0001\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0002\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\n",
      "common-sign-language-0002\n",
      "driver-action-recognition-adas-0002-decoder\n",
      "driver-action-recognition-adas-0002-encoder\n",
      "emotions-recognition-retail-0003\n",
      "face-detection-0200\n",
      "face-detection-0202\n",
      "face-detection-0204\n",
      "face-detection-0205\n",
      "face-detection-0206\n",
      "face-detection-adas-0001\n",
      "face-detection-retail-0004\n",
      "face-detection-retail-0005\n",
      "face-reidentification-retail-0095\n",
      "facial-landmarks-35-adas-0002\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\n",
      "formula-recognition-medium-scan-0001-im2latex-decoder\n",
      "formula-recognition-medium-scan-0001-im2latex-encoder\n",
      "formula-recognition-polynomials-handwritten-0001-decoder\n",
      "formula-recognition-polynomials-handwritten-0001-encoder\n",
      "gaze-estimation-adas-0002\n",
      "handwritten-japanese-recognition-0001\n",
      "handwritten-score-recognition-0003\n",
      "handwritten-simplified-chinese-recognition-0001\n",
      "head-pose-estimation-adas-0001\n",
      "horizontal-text-detection-0001\n",
      "human-pose-estimation-0001\n",
      "human-pose-estimation-0005\n",
      "human-pose-estimation-0006\n",
      "human-pose-estimation-0007\n",
      "icnet-camvid-ava-0001\n",
      "icnet-camvid-ava-sparse-30-0001\n",
      "icnet-camvid-ava-sparse-60-0001\n",
      "image-retrieval-0001\n",
      "instance-segmentation-security-0002\n",
      "instance-segmentation-security-0091\n",
      "instance-segmentation-security-0228\n",
      "instance-segmentation-security-1039\n",
      "instance-segmentation-security-1040\n",
      "landmarks-regression-retail-0009\n",
      "license-plate-recognition-barrier-0001\n",
      "machine-translation-nar-de-en-0002\n",
      "machine-translation-nar-en-de-0002\n",
      "machine-translation-nar-en-ru-0001\n",
      "machine-translation-nar-ru-en-0001\n",
      "noise-suppression-poconetlike-0001\n",
      "pedestrian-and-vehicle-detector-adas-0001\n",
      "pedestrian-detection-adas-0002\n",
      "person-attributes-recognition-crossroad-0230\n",
      "person-attributes-recognition-crossroad-0234\n",
      "person-attributes-recognition-crossroad-0238\n",
      "person-detection-0106\n",
      "person-detection-0200\n",
      "person-detection-0201\n",
      "person-detection-0202\n",
      "person-detection-0203\n",
      "person-detection-action-recognition-0005\n",
      "person-detection-action-recognition-0006\n",
      "person-detection-action-recognition-teacher-0002\n",
      "person-detection-asl-0001\n",
      "person-detection-raisinghand-recognition-0001\n",
      "person-detection-retail-0002\n",
      "person-detection-retail-0013\n",
      "person-reidentification-retail-0277\n",
      "person-reidentification-retail-0286\n",
      "person-reidentification-retail-0287\n",
      "person-reidentification-retail-0288\n",
      "person-vehicle-bike-detection-2000\n",
      "person-vehicle-bike-detection-2001\n",
      "person-vehicle-bike-detection-2002\n",
      "person-vehicle-bike-detection-2003\n",
      "person-vehicle-bike-detection-2004\n",
      "person-vehicle-bike-detection-crossroad-0078\n",
      "person-vehicle-bike-detection-crossroad-1016\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\n",
      "product-detection-0001\n",
      "resnet18-xnor-binary-onnx-0001\n",
      "resnet50-binary-0001\n",
      "road-segmentation-adas-0001\n",
      "semantic-segmentation-adas-0001\n",
      "single-image-super-resolution-1032\n",
      "single-image-super-resolution-1033\n",
      "text-detection-0003\n",
      "text-detection-0004\n",
      "text-image-super-resolution-0001\n",
      "text-recognition-0012\n",
      "text-recognition-0014\n",
      "text-recognition-0015-decoder\n",
      "text-recognition-0015-encoder\n",
      "text-spotting-0005-detector\n",
      "text-spotting-0005-recognizer-decoder\n",
      "text-spotting-0005-recognizer-encoder\n",
      "text-to-speech-en-0001-duration-prediction\n",
      "text-to-speech-en-0001-generation\n",
      "text-to-speech-en-0001-regression\n",
      "text-to-speech-en-multi-0001-duration-prediction\n",
      "text-to-speech-en-multi-0001-generation\n",
      "text-to-speech-en-multi-0001-regression\n",
      "time-series-forecasting-electricity-0001\n",
      "unet-camvid-onnx-0001\n",
      "vehicle-attributes-recognition-barrier-0039\n",
      "vehicle-attributes-recognition-barrier-0042\n",
      "vehicle-detection-0200\n",
      "vehicle-detection-0201\n",
      "vehicle-detection-0202\n",
      "vehicle-detection-adas-0002\n",
      "vehicle-license-plate-detection-barrier-0106\n",
      "weld-porosity-detection-0001\n",
      "yolo-v2-ava-0001\n",
      "yolo-v2-ava-sparse-35-0001\n",
      "yolo-v2-ava-sparse-70-0001\n",
      "yolo-v2-tiny-ava-0001\n",
      "yolo-v2-tiny-ava-sparse-30-0001\n",
      "yolo-v2-tiny-ava-sparse-60-0001\n",
      "yolo-v2-tiny-vehicle-detection-0001\n",
      "Sphereface\n",
      "aclnet\n",
      "aclnet-int8\n",
      "alexnet\n",
      "anti-spoof-mn3\n",
      "bert-base-ner\n",
      "brain-tumor-segmentation-0001\n",
      "brain-tumor-segmentation-0002\n",
      "caffenet\n",
      "cocosnet\n",
      "colorization-siggraph\n",
      "colorization-v2\n",
      "common-sign-language-0001\n",
      "ctdet_coco_dlav0_384\n",
      "ctdet_coco_dlav0_512\n",
      "ctpn\n",
      "deblurgan-v2\n",
      "deeplabv3\n",
      "densenet-121\n",
      "densenet-121-caffe2\n",
      "densenet-121-tf\n",
      "densenet-161\n",
      "densenet-161-tf\n",
      "densenet-169\n",
      "densenet-169-tf\n",
      "densenet-201\n",
      "densenet-201-tf\n",
      "dla-34\n",
      "efficientdet-d0-tf\n",
      "efficientdet-d1-tf\n",
      "efficientnet-b0\n",
      "efficientnet-b0-pytorch\n",
      "efficientnet-b0_auto_aug\n",
      "efficientnet-b5\n",
      "efficientnet-b5-pytorch\n",
      "efficientnet-b7-pytorch\n",
      "efficientnet-b7_auto_aug\n",
      "f3net\n",
      "face-detection-retail-0044\n",
      "face-recognition-resnet100-arcface-onnx\n",
      "faceboxes-pytorch\n",
      "facenet-20180408-102900\n",
      "fast-neural-style-mosaic-onnx\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\n",
      "faster_rcnn_inception_v2_coco\n",
      "faster_rcnn_resnet101_coco\n",
      "faster_rcnn_resnet50_coco\n",
      "fastseg-large\n",
      "fastseg-small\n",
      "fcrn-dp-nyu-depth-v2-tf\n",
      "forward-tacotron-duration-prediction\n",
      "forward-tacotron-regression\n",
      "gmcnn-places2-tf\n",
      "googlenet-v1\n",
      "googlenet-v1-tf\n",
      "googlenet-v2\n",
      "googlenet-v2-tf\n",
      "googlenet-v3\n",
      "googlenet-v3-pytorch\n",
      "googlenet-v4-tf\n",
      "hbonet-0.25\n",
      "hbonet-0.5\n",
      "hbonet-1.0\n",
      "higher-hrnet-w32-human-pose-estimation\n",
      "hrnet-v2-c1-segmentation\n",
      "human-pose-estimation-3d-0001\n",
      "i3d-rgb-tf\n",
      "inception-resnet-v2-tf\n",
      "license-plate-recognition-barrier-0007\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\n",
      "mask_rcnn_inception_v2_coco\n",
      "mask_rcnn_resnet101_atrous_coco\n",
      "mask_rcnn_resnet50_atrous_coco\n",
      "midasnet\n",
      "mixnet-l\n",
      "mobilefacedet-v1-mxnet\n",
      "mobilenet-ssd\n",
      "mobilenet-v1-0.25-128\n",
      "mobilenet-v1-0.50-160\n",
      "mobilenet-v1-0.50-224\n",
      "mobilenet-v1-1.0-224\n",
      "mobilenet-v1-1.0-224-tf\n",
      "mobilenet-v2\n",
      "mobilenet-v2-1.0-224\n",
      "mobilenet-v2-1.4-224\n",
      "mobilenet-v2-pytorch\n",
      "mobilenet-v3-large-1.0-224-tf\n",
      "mobilenet-v3-small-1.0-224-tf\n",
      "mozilla-deepspeech-0.6.1\n",
      "mozilla-deepspeech-0.8.2\n",
      "mtcnn-o\n",
      "mtcnn-p\n",
      "mtcnn-r\n",
      "netvlad-tf\n",
      "nfnet-f0\n",
      "octave-densenet-121-0.125\n",
      "octave-resnet-101-0.125\n",
      "octave-resnet-200-0.125\n",
      "octave-resnet-26-0.25\n",
      "octave-resnet-50-0.125\n",
      "octave-resnext-101-0.25\n",
      "octave-resnext-50-0.25\n",
      "octave-se-resnet-50-0.125\n",
      "open-closed-eye-0001\n",
      "pelee-coco\n",
      "pspnet-pytorch\n",
      "quartznet-15x5-en\n",
      "regnetx-3.2gf\n",
      "repvgg-a0\n",
      "repvgg-b1\n",
      "repvgg-b3\n",
      "resnest-50-pytorch\n",
      "resnet-18-pytorch\n",
      "resnet-34-pytorch\n",
      "resnet-50-caffe2\n",
      "resnet-50-pytorch\n",
      "resnet-50-tf\n",
      "retinaface-resnet50-pytorch\n",
      "retinanet-tf\n",
      "rexnet-v1-x1.0\n",
      "rfcn-resnet101-coco-tf\n",
      "se-inception\n",
      "se-resnet-101\n",
      "se-resnet-152\n",
      "se-resnet-50\n",
      "se-resnext-101\n",
      "se-resnext-50\n",
      "shufflenet-v2-x1.0\n",
      "single-human-pose-estimation-0001\n",
      "squeezenet1.0\n",
      "squeezenet1.1\n",
      "squeezenet1.1-caffe2\n",
      "ssd-resnet34-1200-onnx\n",
      "ssd300\n",
      "ssd512\n",
      "ssd_mobilenet_v1_coco\n",
      "ssd_mobilenet_v1_fpn_coco\n",
      "ssd_mobilenet_v2_coco\n",
      "ssd_resnet50_v1_fpn_coco\n",
      "ssdlite_mobilenet_v2\n",
      "text-recognition-resnet-fc\n",
      "ultra-lightweight-face-detection-rfb-320\n",
      "ultra-lightweight-face-detection-slim-320\n",
      "vehicle-license-plate-detection-barrier-0123\n",
      "vehicle-reid-0001\n",
      "vgg16\n",
      "vgg19\n",
      "vgg19-caffe2\n",
      "wavernn-rnn\n",
      "wavernn-upsampler\n",
      "yolact-resnet50-fpn-pytorch\n",
      "yolo-v1-tiny-tf\n",
      "yolo-v2-tf\n",
      "yolo-v2-tiny-tf\n",
      "yolo-v3-tf\n",
      "yolo-v3-tiny-tf\n",
      "yolo-v4-tf\n",
      "yolo-v4-tiny-tf\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6d3db",
   "metadata": {},
   "source": [
    "### Download Intel Pre-trained Model\n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the --name argument to download Intel pre-trained Model person-vehicle-bike-detection-crossroad-0078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7f9c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading person-vehicle-bike-detection-crossroad-0078 ||################\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP32/person-vehicle-bike-detection-crossroad-0078.xml\n",
      "... 100%, 372 KB, 679 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP32/person-vehicle-bike-detection-crossroad-0078.bin\n",
      "... 100%, 4603 KB, 4032 KB/s, 1 seconds passed\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP16/person-vehicle-bike-detection-crossroad-0078.xml\n",
      "... 100%, 372 KB, 688 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP16/person-vehicle-bike-detection-crossroad-0078.bin\n",
      "... 100%, 2301 KB, 2405 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml\n",
      "... 100%, 1056 KB, 1287 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.bin\n",
      "... 100%, 1199 KB, 1460 KB/s, 0 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --name person-vehicle-bike-detection-crossroad-0078 -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf0558",
   "metadata": {},
   "source": [
    "### Get Model Information\n",
    "Run the following cell to get the information of the model person-vehicle-bike-detection-crossroad-0078 which was downloaded in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683f598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"name\": \"person-vehicle-bike-detection-crossroad-0078\",\r\n",
      "        \"composite_model_name\": null,\r\n",
      "        \"description\": \"Multiclass (person -  vehicle -  non-vehicle) detector based on SSD detection architecture -  RMNet backbone and learnable image downscale block (person-vehicle-bike-detection-crossroad-0066 with extra pooling)\",\r\n",
      "        \"framework\": \"dldt\",\r\n",
      "        \"license_url\": \"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE\",\r\n",
      "        \"precisions\": [\r\n",
      "            \"FP16\",\r\n",
      "            \"FP16-INT8\",\r\n",
      "            \"FP32\"\r\n",
      "        ],\r\n",
      "        \"quantization_output_precisions\": [],\r\n",
      "        \"subdirectory\": \"intel/person-vehicle-bike-detection-crossroad-0078\",\r\n",
      "        \"task_type\": \"detection\"\r\n",
      "    }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!info_dumper.py --name person-vehicle-bike-detection-crossroad-0078"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876cf54",
   "metadata": {},
   "source": [
    "### Download Public Pre-trained Model\n",
    "Run the following cell to run the Model Downloader utility with the --name argument to download Public pre-trained Model mobilenet-ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf4ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading mobilenet-ssd ||################\n",
      "\n",
      "========== Downloading models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "... 100%, 28 KB, 44871 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n",
      "... 100%, 22605 KB, 22144 KB/s, 1 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --name mobilenet-ssd -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44afb17",
   "metadata": {},
   "source": [
    "### Get Model Information\n",
    "Run the following cell to get the information of the Intel Public model mobilenet-ssd which was downloaded in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a257dd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"name\": \"mobilenet-ssd\",\r\n",
      "        \"composite_model_name\": null,\r\n",
      "        \"description\": \"The \\\"mobilenet-ssd\\\" model is a Single-Shot multibox Detection (SSD) network intended to perform object detection. This model is implemented using the Caffe* framework. For details about this model, check out the repository <https://github.com/chuanqi305/MobileNet-SSD>.\\nThe model input is a blob that consists of a single image of \\\"1, 3, 300, 300\\\" in \\\"BGR\\\" order, also like the \\\"densenet-121\\\" model. The BGR mean values need to be subtracted as follows: [127.5, 127.5, 127.5] before passing the image blob into the network. In addition, values must be divided by 0.007843.\\nThe model output is a typical vector containing the tracked object data, as previously described.\",\r\n",
      "        \"framework\": \"caffe\",\r\n",
      "        \"license_url\": \"https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/LICENSE\",\r\n",
      "        \"precisions\": [\r\n",
      "            \"FP16\",\r\n",
      "            \"FP32\"\r\n",
      "        ],\r\n",
      "        \"quantization_output_precisions\": [\r\n",
      "            \"FP16-INT8\",\r\n",
      "            \"FP32-INT8\"\r\n",
      "        ],\r\n",
      "        \"subdirectory\": \"public/mobilenet-ssd\",\r\n",
      "        \"task_type\": \"detection\"\r\n",
      "    }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!info_dumper.py --name mobilenet-ssd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46218750",
   "metadata": {},
   "source": [
    "Congratulations! You have completed this lab excercise by downloading a model from Open Model Zoo. Now the next step is to optimize this model. If you download a pre-trained model by Intel, it will be already optimized and converted to IR format and hence ready to use for inferencing using the Inference Engine in your AI application. If you download a public model from Open Model Zoo or use your own model, you will need to convert it to IR format using the Model Optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.4 LTS)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
