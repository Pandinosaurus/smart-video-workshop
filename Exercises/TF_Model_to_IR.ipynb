{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8b3720",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial lets understand how to convert TensorFlow* Object Detection API Models to the OpenVINO™ toolkit Intermediate represenstation (IR) format. For detailed explanation refer [Converting TensorFlow Object Detection API Models](https://docs.openvinotoolkit.org/2021.1/openvino_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html).\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cb991",
   "metadata": {},
   "source": [
    "# Download the model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility  to download some common inference models from the [Open Model Zoo](https://github.com/opencv/open_model_zoo). \n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the `--print_all` argument to see all the available inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1ec2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action-recognition-0001-decoder\r\n",
      "action-recognition-0001-encoder\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "asl-recognition-0004\r\n",
      "bert-large-uncased-whole-word-masking-squad-0001\r\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\r\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-0002\r\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\r\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\r\n",
      "common-sign-language-0002\r\n",
      "driver-action-recognition-adas-0002-decoder\r\n",
      "driver-action-recognition-adas-0002-encoder\r\n",
      "emotions-recognition-retail-0003\r\n",
      "face-detection-0200\r\n",
      "face-detection-0202\r\n",
      "face-detection-0204\r\n",
      "face-detection-0205\r\n",
      "face-detection-0206\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0005\r\n",
      "face-reidentification-retail-0095\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\r\n",
      "formula-recognition-medium-scan-0001-im2latex-decoder\r\n",
      "formula-recognition-medium-scan-0001-im2latex-encoder\r\n",
      "formula-recognition-polynomials-handwritten-0001-decoder\r\n",
      "formula-recognition-polynomials-handwritten-0001-encoder\r\n",
      "gaze-estimation-adas-0002\r\n",
      "handwritten-japanese-recognition-0001\r\n",
      "handwritten-score-recognition-0003\r\n",
      "handwritten-simplified-chinese-recognition-0001\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "horizontal-text-detection-0001\r\n",
      "human-pose-estimation-0001\r\n",
      "human-pose-estimation-0005\r\n",
      "human-pose-estimation-0006\r\n",
      "human-pose-estimation-0007\r\n",
      "icnet-camvid-ava-0001\r\n",
      "icnet-camvid-ava-sparse-30-0001\r\n",
      "icnet-camvid-ava-sparse-60-0001\r\n",
      "image-retrieval-0001\r\n",
      "instance-segmentation-security-0002\r\n",
      "instance-segmentation-security-0091\r\n",
      "instance-segmentation-security-0228\r\n",
      "instance-segmentation-security-1039\r\n",
      "instance-segmentation-security-1040\r\n",
      "landmarks-regression-retail-0009\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "machine-translation-nar-de-en-0002\r\n",
      "machine-translation-nar-en-de-0002\r\n",
      "machine-translation-nar-en-ru-0001\r\n",
      "machine-translation-nar-ru-en-0001\r\n",
      "noise-suppression-poconetlike-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-attributes-recognition-crossroad-0234\r\n",
      "person-attributes-recognition-crossroad-0238\r\n",
      "person-detection-0106\r\n",
      "person-detection-0200\r\n",
      "person-detection-0201\r\n",
      "person-detection-0202\r\n",
      "person-detection-0203\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0006\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-asl-0001\r\n",
      "person-detection-raisinghand-recognition-0001\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0013\r\n",
      "person-reidentification-retail-0277\r\n",
      "person-reidentification-retail-0286\r\n",
      "person-reidentification-retail-0287\r\n",
      "person-reidentification-retail-0288\r\n",
      "person-vehicle-bike-detection-2000\r\n",
      "person-vehicle-bike-detection-2001\r\n",
      "person-vehicle-bike-detection-2002\r\n",
      "person-vehicle-bike-detection-2003\r\n",
      "person-vehicle-bike-detection-2004\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-1016\r\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\r\n",
      "product-detection-0001\r\n",
      "resnet18-xnor-binary-onnx-0001\r\n",
      "resnet50-binary-0001\r\n",
      "road-segmentation-adas-0001\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1033\r\n",
      "text-detection-0003\r\n",
      "text-detection-0004\r\n",
      "text-image-super-resolution-0001\r\n",
      "text-recognition-0012\r\n",
      "text-recognition-0014\r\n",
      "text-recognition-0015-decoder\r\n",
      "text-recognition-0015-encoder\r\n",
      "text-spotting-0005-detector\r\n",
      "text-spotting-0005-recognizer-decoder\r\n",
      "text-spotting-0005-recognizer-encoder\r\n",
      "text-to-speech-en-0001-duration-prediction\r\n",
      "text-to-speech-en-0001-generation\r\n",
      "text-to-speech-en-0001-regression\r\n",
      "text-to-speech-en-multi-0001-duration-prediction\r\n",
      "text-to-speech-en-multi-0001-generation\r\n",
      "text-to-speech-en-multi-0001-regression\r\n",
      "time-series-forecasting-electricity-0001\r\n",
      "unet-camvid-onnx-0001\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-attributes-recognition-barrier-0042\r\n",
      "vehicle-detection-0200\r\n",
      "vehicle-detection-0201\r\n",
      "vehicle-detection-0202\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "weld-porosity-detection-0001\r\n",
      "yolo-v2-ava-0001\r\n",
      "yolo-v2-ava-sparse-35-0001\r\n",
      "yolo-v2-ava-sparse-70-0001\r\n",
      "yolo-v2-tiny-ava-0001\r\n",
      "yolo-v2-tiny-ava-sparse-30-0001\r\n",
      "yolo-v2-tiny-ava-sparse-60-0001\r\n",
      "yolo-v2-tiny-vehicle-detection-0001\r\n",
      "Sphereface\r\n",
      "aclnet\r\n",
      "aclnet-int8\r\n",
      "alexnet\r\n",
      "anti-spoof-mn3\r\n",
      "bert-base-ner\r\n",
      "brain-tumor-segmentation-0001\r\n",
      "brain-tumor-segmentation-0002\r\n",
      "caffenet\r\n",
      "cocosnet\r\n",
      "colorization-siggraph\r\n",
      "colorization-v2\r\n",
      "common-sign-language-0001\r\n",
      "ctdet_coco_dlav0_384\r\n",
      "ctdet_coco_dlav0_512\r\n",
      "ctpn\r\n",
      "deblurgan-v2\r\n",
      "deeplabv3\r\n",
      "densenet-121\r\n",
      "densenet-121-caffe2\r\n",
      "densenet-121-tf\r\n",
      "densenet-161\r\n",
      "densenet-161-tf\r\n",
      "densenet-169\r\n",
      "densenet-169-tf\r\n",
      "densenet-201\r\n",
      "densenet-201-tf\r\n",
      "dla-34\r\n",
      "efficientdet-d0-tf\r\n",
      "efficientdet-d1-tf\r\n",
      "efficientnet-b0\r\n",
      "efficientnet-b0-pytorch\r\n",
      "efficientnet-b0_auto_aug\r\n",
      "efficientnet-b5\r\n",
      "efficientnet-b5-pytorch\r\n",
      "efficientnet-b7-pytorch\r\n",
      "efficientnet-b7_auto_aug\r\n",
      "f3net\r\n",
      "face-detection-retail-0044\r\n",
      "face-recognition-resnet100-arcface-onnx\r\n",
      "faceboxes-pytorch\r\n",
      "facenet-20180408-102900\r\n",
      "fast-neural-style-mosaic-onnx\r\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "faster_rcnn_inception_v2_coco\r\n",
      "faster_rcnn_resnet101_coco\r\n",
      "faster_rcnn_resnet50_coco\r\n",
      "fastseg-large\r\n",
      "fastseg-small\r\n",
      "fcrn-dp-nyu-depth-v2-tf\r\n",
      "forward-tacotron-duration-prediction\r\n",
      "forward-tacotron-regression\r\n",
      "gmcnn-places2-tf\r\n",
      "googlenet-v1\r\n",
      "googlenet-v1-tf\r\n",
      "googlenet-v2\r\n",
      "googlenet-v2-tf\r\n",
      "googlenet-v3\r\n",
      "googlenet-v3-pytorch\r\n",
      "googlenet-v4-tf\r\n",
      "hbonet-0.25\r\n",
      "hbonet-0.5\r\n",
      "hbonet-1.0\r\n",
      "higher-hrnet-w32-human-pose-estimation\r\n",
      "hrnet-v2-c1-segmentation\r\n",
      "human-pose-estimation-3d-0001\r\n",
      "i3d-rgb-tf\r\n",
      "inception-resnet-v2-tf\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "mask_rcnn_inception_v2_coco\r\n",
      "mask_rcnn_resnet101_atrous_coco\r\n",
      "mask_rcnn_resnet50_atrous_coco\r\n",
      "midasnet\r\n",
      "mixnet-l\r\n",
      "mobilefacedet-v1-mxnet\r\n",
      "mobilenet-ssd\r\n",
      "mobilenet-v1-0.25-128\r\n",
      "mobilenet-v1-0.50-160\r\n",
      "mobilenet-v1-0.50-224\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v1-1.0-224-tf\r\n",
      "mobilenet-v2\r\n",
      "mobilenet-v2-1.0-224\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "mobilenet-v2-pytorch\r\n",
      "mobilenet-v3-large-1.0-224-tf\r\n",
      "mobilenet-v3-small-1.0-224-tf\r\n",
      "mozilla-deepspeech-0.6.1\r\n",
      "mozilla-deepspeech-0.8.2\r\n",
      "mtcnn-o\r\n",
      "mtcnn-p\r\n",
      "mtcnn-r\r\n",
      "netvlad-tf\r\n",
      "nfnet-f0\r\n",
      "octave-densenet-121-0.125\r\n",
      "octave-resnet-101-0.125\r\n",
      "octave-resnet-200-0.125\r\n",
      "octave-resnet-26-0.25\r\n",
      "octave-resnet-50-0.125\r\n",
      "octave-resnext-101-0.25\r\n",
      "octave-resnext-50-0.25\r\n",
      "octave-se-resnet-50-0.125\r\n",
      "open-closed-eye-0001\r\n",
      "pelee-coco\r\n",
      "pspnet-pytorch\r\n",
      "quartznet-15x5-en\r\n",
      "regnetx-3.2gf\r\n",
      "repvgg-a0\r\n",
      "repvgg-b1\r\n",
      "repvgg-b3\r\n",
      "resnest-50-pytorch\r\n",
      "resnet-18-pytorch\r\n",
      "resnet-34-pytorch\r\n",
      "resnet-50-caffe2\r\n",
      "resnet-50-pytorch\r\n",
      "resnet-50-tf\r\n",
      "retinaface-resnet50-pytorch\r\n",
      "retinanet-tf\r\n",
      "rexnet-v1-x1.0\r\n",
      "rfcn-resnet101-coco-tf\r\n",
      "se-inception\r\n",
      "se-resnet-101\r\n",
      "se-resnet-152\r\n",
      "se-resnet-50\r\n",
      "se-resnext-101\r\n",
      "se-resnext-50\r\n",
      "shufflenet-v2-x1.0\r\n",
      "single-human-pose-estimation-0001\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "squeezenet1.1-caffe2\r\n",
      "ssd-resnet34-1200-onnx\r\n",
      "ssd300\r\n",
      "ssd512\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "ssd_mobilenet_v1_fpn_coco\r\n",
      "ssd_mobilenet_v2_coco\r\n",
      "ssd_resnet50_v1_fpn_coco\r\n",
      "ssdlite_mobilenet_v2\r\n",
      "text-recognition-resnet-fc\r\n",
      "ultra-lightweight-face-detection-rfb-320\r\n",
      "ultra-lightweight-face-detection-slim-320\r\n",
      "vehicle-license-plate-detection-barrier-0123\r\n",
      "vehicle-reid-0001\r\n",
      "vgg16\r\n",
      "vgg19\r\n",
      "vgg19-caffe2\r\n",
      "wavernn-rnn\r\n",
      "wavernn-upsampler\r\n",
      "yolact-resnet50-fpn-pytorch\r\n",
      "yolo-v1-tiny-tf\r\n",
      "yolo-v2-tf\r\n",
      "yolo-v2-tiny-tf\r\n",
      "yolo-v3-tf\r\n",
      "yolo-v3-tiny-tf\r\n",
      "yolo-v4-tf\r\n",
      "yolo-v4-tiny-tf\r\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcbc93",
   "metadata": {},
   "source": [
    "\n",
    "Following cell will downlaod SSD MobileNet v2 public model from [Open Model Zoo](https://github.com/opencv/open_model_zoo) and extract in ssd_mobilenet_v2_coco folder in your working directory. The required model file is **ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869ccc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading ssd_mobilenet_v2_coco ||################\n",
      "\n",
      "========== Downloading ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco.tar.gz\n",
      "... 100%, 183521 KB, 23520 KB/s, 7 seconds passed\n",
      "\n",
      "========== Unpacking ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco.tar.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --name ssd_mobilenet_v2_coco -o ssd_mobilenet_v2_coco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fab5c",
   "metadata": {},
   "source": [
    "# Convert the frozen TensorFlow* model to IR format:\n",
    "\n",
    "If you have already installed Intel® Distribution of OpenVINO™ toolkit in you local machine, Model Optimizer will abe available in \\<INSTALL_DIR\\>/deployment_tools/model_optimizer directory. To convert an TensorFlow* model you can use following command.\n",
    "\n",
    "python3 /<INSTALL_DIR\\>/deployment_tools/model_optimizer/mo.py /\n",
    "\n",
    "--input_model=\\<TensorFlow model path\\>/frozen_inference_graph.pb /\n",
    "\n",
    "--transformations_config /<INSTALL_DIR\\>/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json /\n",
    "\n",
    "--tensorflow_object_detection_api_pipeline_config \\<TensorFlow model path\\>/pipeline.config /\n",
    "\n",
    "--reverse_input_channels\n",
    "\n",
    "Here,\n",
    "- **--input_model:** Path for the input model\n",
    "- **--transformations_config:**  A subgraph replacement configuration json file with transformations description. For the models downloaded from the TensorFlow* Object Detection API zoo, you can find the configuration files in the <INSTALL_DIR>/deployment_tools/model_optimizer/extensions/front/tf directory.\n",
    "- **--tensorflow_object_detection_api_pipeline_config:**  A special configuration file that describes the topology hyper-parameters and structure of the TensorFlow* Object Detection API model. For the models downloaded from the TensorFlow* Object Detection API zoo, the configuration file is named pipeline.config. If you plan to train a model yourself, you can find templates for these files in the [models repository](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs).\n",
    "- **--reverse_input_channels:** The color channel order (RGB or BGR) of an input data should match the channel order of the model training dataset. Otherwise, inference results may be incorrect. If you convert a TensorFlow* Object Detection API model to use with the Inference Engine sample applications, you must specify the this parameter.\n",
    "\n",
    "For other Tensorflow* specific parameters refer [TensorFlow* Specific Conversion Parameters](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#tensorflow_specific_conversion_params). \n",
    "Following cell will convert the model to IR format with FP32 precision. It might take few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9734317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u80020/ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
      "\t- Path for generated IR: \t/home/u80020/.\n",
      "\t- IR output name: \tfrozen_inference_graph\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tTrue\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \t/home/u80020/ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config\n",
      "\t- Use the config file: \tNone\n",
      "\t- Inference Engine found in: \t/opt/intel/openvino/python/python3.6/openvino\n",
      "Inference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u80020/frozen_inference_graph.xml\n",
      "[ SUCCESS ] BIN file: /home/u80020/frozen_inference_graph.bin\n",
      "[ SUCCESS ] Total execution time: 45.44 seconds. \n",
      "[ SUCCESS ] Memory consumed: 661 MB. \n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py --input_model=ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb --transformations_config /opt/intel/openvino_2021/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config ssd_mobilenet_v2_coco/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config --reverse_input_channels --data_type FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e7a34",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully created the IR files and learnt it was a very easy one time process. You now have an optimized model. We will use these optimized model(IR files) in the AI application for inferencing which will be done with the help of Inference Engine. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.4 LTS)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
